{"nbformat_minor": 2, "cells": [{"execution_count": 22, "cell_type": "code", "source": "import time\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom pyspark.sql.types import ArrayType, StringType, DateType, IntegerType, DoubleType\nfrom pyspark.sql.functions import col, udf, monotonically_increasing_id, lag, datediff, collect_list, desc\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover\nfrom pyspark.mllib.clustering import LDA, LDAModel\nfrom pyspark.mllib.linalg import SparseVector, DenseVector, Vectors, VectorUDT\nfrom pyspark.sql.window import Window\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": "query = '''\nselect text from orestist.conf_calls_sec \nwhere conf_calls_sec.section = 'man'\nand conf_calls_sec.title = 'Q4 2003 Earnings Call'\nlimit 5\n\n'''\n\ndf = sqlContext.sql(query)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 27, "cell_type": "code", "source": "# # numeric vector\n\n# cv = CountVectorizer()\\\n#     .setInputCol(\"words\")\\\n#     .setOutputCol(\"countVec\")\\\n#     .setVocabSize(200)\\\n#     .setMinTF(1)\\\n#     .setMinDF(1)\n# fittedCV = cv.fit(tokenized)\n# fittedCV.transform(tokenized).show()\n# prepped = cvfitted.transform(tokenized)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 28, "cell_type": "code", "source": "tkn = RegexTokenizer().setInputCol(\"text\").setOutputCol(\"tokens\")\ntokenized = tkn.transform(df.select(\"text\"))\ntokenized.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+\n|                text|              tokens|\n+--------------------+--------------------+\n|good day ladies a...|[good, day, ladie...|\n|good morning and ...|[good, morning, a...|\n|good morning ladi...|[good, morning, l...|\n|good afternoon my...|[good, afternoon,...|\n|good morning ladi...|[good, morning, l...|\n+--------------------+--------------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 29, "cell_type": "code", "source": "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\nstops =  StopWordsRemover()\\\n    .setStopWords(englishStopWords)\\\n    .setInputCol(\"tokens\") \\\n    .setOutputCol(\"words\")\ntokenized = stops.transform(tokenized.select(\"tokens\"))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 30, "cell_type": "code", "source": "# from pyspark.ml.feature import NGram\n# unigram = NGram().setInputCol(\"words\").setN(1)\n# bigram = NGram().setInputCol(\"words\").setN(2)\n# unigram.transform(tokenized.select(\"words\")).show()\n# bigram.transform(tokenized.select(\"words\")).show()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 31, "cell_type": "code", "source": "#applying LDA - to imput text data into LDA convert to numeric format using countvector \n\ncv = CountVectorizer()\\\n    .setInputCol(\"words\")\\\n    .setOutputCol(\"features\")\\\n    .setVocabSize(200)\\\n    .setMinTF(1)\\\n    .setMinDF(1)\\\n    .setBinary(True)\ncvfitted = cv.fit(tokenized)\nprepped = cvfitted.transform(tokenized)\n\n", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 32, "cell_type": "code", "source": "from pyspark.ml.clustering import LDA\nlda = LDA().setK(5).setMaxIter(5) # K is the number of topics \nlda.explainParams()\nmodel = lda.fit(prepped)\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 33, "cell_type": "code", "source": "model.describeTopics(5).show()\nvocab = cvfitted.vocabulary #vocab is a list", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+--------------------+\n|topic|         termIndices|         termWeights|\n+-----+--------------------+--------------------+\n|    0|[117, 78, 9, 26, ...|[0.00645118874944...|\n|    1|[15, 175, 192, 63...|[0.00613633039669...|\n|    2|[40, 77, 128, 192...|[0.00615479010200...|\n|    3|[24, 135, 148, 9,...|[0.00610157778374...|\n|    4|[146, 62, 64, 110...|[0.00619146191743...|\n+-----+--------------------+--------------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 37, "cell_type": "code", "source": "# #Output topics. Each is a distribution over words (matching word count vectors)\n# print(\"Learned topics (as distributions over vocab of \" + str(model.vocabSize())\n#       + \" words):\")\n# topics = model.topicsMatrix().toArray() #this is an array\n\n# #print(topics)\n\n# for i in range(1,200):\n#     print(vocab[i])\n#     print(topics[i])\n    \n\n# for topic in range(5):\n#     print(\"Topic \" + str(topic) + \":\")\n    \n#     #for word in range(0, 200):\n#         #print(word)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 38, "cell_type": "code", "source": "def top_words_for_topic(topic_index, num_top_words=10):\n    df_topic = pd.DataFrame.from_dict({\n        'word': cvfitted.vocabulary,\n        'weight': [topics[word_index][topic_index] for word_index in range(model.vocabSize())]\n    })\n    return df_topic.sort_values('weight', ascending=False).iloc[0:num_top_words+1]", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 39, "cell_type": "code", "source": "#lda_model = LDA.train(corpus, k=num_topics)\nnum_topics = 5\nnum_top_words = 20\ndfs_topics = []\nfor topic_index in range(num_topics):\n    df_topic = top_words_for_topic(topic_index=topic_index, num_top_words=num_top_words)\n    dfs_topics.append(df_topic.reset_index()[['word']]\\\n                              .rename(columns={'word': 'topic_' + str(topic_index + 1)}))\ndf_topics = pd.concat(dfs_topics, axis=1)\ndf_topics", "outputs": [{"output_type": "stream", "name": "stdout", "text": "         topic_1    topic_2    topic_3   topic_4      topic_5\n0         global          5      lower  products         imrt\n1   acquisitions   spending      level  estimate    netscreen\n2       earnings  questions    looking        16         2002\n3              2  september  questions  earnings          tax\n4           june  increased       fire      cash         mtvr\n5       compared   products    backlog      mtvr       demand\n6       wireless      thank   wireless   welcome      quarter\n7             30     demand    defense   believe        based\n8        another       last      costs   average          per\n9      estimates    systems      focus       one    operating\n10     netscreen    results    forward    growth      looking\n11       systems    netcool          0         2         june\n12    government          1    diluted  oncology      segment\n13        period     income     fourth  recovery  significant\n14   improvement      major      forma    annual         fire\n15         forma    company      large     gross       fiscal\n16        versus     orders       john     gases   government\n17          mtvr        net  announced        us    effective\n18           let     expect       page       tax        large\n19         three  announced       imrt    orders            0\n20        demand   earnings  estimates  guidance         line"}], "metadata": {"collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "# topics = model.describeTopics(maxTermsPerTopic = 15)\n# for x, topic in enumerate(topics):\n#     print ('topic nr: ' + str(x))\n#     words = topic[0]\n#     weights = topic[1]\n#     for n in range(len(words)):\n#         print (cvmodel.vocabulary[words[n]] + ' ' + str(weights[n]))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}